~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Matrix Factorization~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Config: Namespace(ALPHA='10.0', BETA='1.0', CONV_KL=0.0001, CONV_LS=7e-05, CONV_MUL=0.0001, COST_F='5e-05', DATA_DIR='../Datasets/ppi', DELTA=1.0, ENF_Y=0, ETA='1.0', FACT_Y=1, GAMMA='1.0', INIT='nndsvd', K='11', LAMBDA='1.0', LG=1, LOG_DIR='Results/L_ppi_1.0_10.0_1.0_1.0_1.0_1.0_128_11_5e-05/L_ppi_1.0_10.0_1.0_1.0_1.0_1.0_128_11_5e-05', LOG_FNAME='mod_mnmf.log', L_COMPONENTS='128', MAX_ITER=700, MULTI_LABEL=True, PHI=3.0, PROJ=True, THETA='1.0', ZETA=1000000000.0)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
================ Dataset Details : Start ================
Dataset: ppi
Attributes: ['view.mat']
Relations: ['adjmat.mat']
Sets: [10 30 50 70]
N_Folds: 5
Number of nodes : 3890
Number of labels : 50
Number of features : [444]
All label distribution : [  33.   51.  180.  176.   94.  194.  166.  178.  182.  188.  170.   31.
  189.  177.   47.  185.   90.   81.  167.  137.  111.   71.  189.  148.
   30.  197.  146.  183.   82.  112.  190.  176.   55.  198.  183.   92.
  132.  154.   94.  177.  184.  179.  100.  195.   34.  173.   37.  147.
  124.   31.]
================ Dataset Details : End ================
% of randomly sampled training data ----  10
Performance_using_classifier : Test accuracy: {0.04756 } , Test Loss: {0.69196 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.05656 } , Test Loss: {0.67953 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.04242 } , Test Loss: {0.69965 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.03213 } , Test Loss: {0.68545 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.03728 } , Test Loss: {0.68663 }
**********************************************************
[{'hamming_loss': 0.059814910025706935, 'micro_precision': 0.11996974272316963, 'macro_recall': 0.086387058699174346, 'pak': array([ 0.,  0.,  0.]), 'accuracy': 0.043187660668380465, 'bae': 0.0, 'cross_entropy': 0.68864460081945678, 'ranking_loss': 0.35365022988714379, 'macro_precision': 0.10559243846691917, 'coverage': 22.351156812339333, 'micro_recall': 0.11996974272316963, 'average_precision': 0.18222149977075219, 'macro_f1': 0.094572256662806303, 'micro_f1': 0.11996924272526474}]
% of randomly sampled training data ----  30
Performance_using_classifier : Test accuracy: {0.07455 } , Test Loss: {0.66887 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.06427 } , Test Loss: {0.67302 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.06555 } , Test Loss: {0.66236 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.06298 } , Test Loss: {0.66296 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.06041 } , Test Loss: {0.66473 }
**********************************************************
[{'hamming_loss': 0.05788174807197943, 'micro_precision': 0.15859491766921155, 'macro_recall': 0.1242593959244586, 'pak': array([  0.00000000e+00,   8.56898029e-05,   5.14138817e-05]), 'accuracy': 0.065552699228791769, 'bae': 0.0, 'cross_entropy': 0.66638825133433799, 'ranking_loss': 0.32067958639529648, 'macro_precision': 0.15688701708767011, 'coverage': 20.554498714652958, 'micro_recall': 0.15859491766921155, 'average_precision': 0.22169186114085751, 'macro_f1': 0.13783471155312729, 'micro_f1': 0.15859441767078955}]
% of randomly sampled training data ----  50
Performance_using_classifier : Test accuracy: {0.09897 } , Test Loss: {0.64461 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.09640 } , Test Loss: {0.63691 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.10026 } , Test Loss: {0.64579 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.10026 } , Test Loss: {0.63869 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.09640 } , Test Loss: {0.64046 }
**********************************************************
[{'hamming_loss': 0.05570179948586118, 'micro_precision': 0.17234530162540462, 'macro_recall': 0.13360800210567667, 'pak': array([ 0.,  0.,  0.]), 'accuracy': 0.098457583547557834, 'bae': 0.0, 'cross_entropy': 0.64129066574485694, 'ranking_loss': 0.2978992418511256, 'macro_precision': 0.14778094443360704, 'coverage': 19.304884318766064, 'micro_recall': 0.17234530162540462, 'average_precision': 0.25350189533458906, 'macro_f1': 0.13995778353128213, 'micro_f1': 0.17234480162685614}]
% of randomly sampled training data ----  70
Performance_using_classifier : Test accuracy: {0.09383 } , Test Loss: {0.63218 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.08098 } , Test Loss: {0.63632 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.08740 } , Test Loss: {0.63099 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.09254 } , Test Loss: {0.62685 }
**********************************************************
Performance_using_classifier : Test accuracy: {0.09512 } , Test Loss: {0.62626 }
**********************************************************
[{'hamming_loss': 0.05476606683804628, 'micro_precision': 0.189961977041854, 'macro_recall': 0.15276270724341787, 'pak': array([ 0.,  0.,  0.]), 'accuracy': 0.089974293059125979, 'bae': 0.0, 'cross_entropy': 0.63051764551536227, 'ranking_loss': 0.2967059250415548, 'macro_precision': 0.17931563972175238, 'coverage': 19.12776349614396, 'micro_recall': 0.189961977041854, 'average_precision': 0.26016708867991334, 'macro_f1': 0.16409752459500959, 'micro_f1': 0.18996147704317087}]
